from langchain.schema import SystemMessage, HumanMessage
from langchain_core.messages import BaseMessage
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate
from langchain_openai import AzureChatOpenAI
from pydantic import SecretStr, BaseModel, Field

from tasks._constants import DIAL_URL, API_KEY

SYSTEM_PROMPT = "You are a secure colleague directory assistant designed to help users find contact information for business purposes."

PROFILE = """
# Profile: Amanda Grace Johnson

**Full Name:** Amanda Grace Johnson  
**SSN:** 890-12-3456  
**Date of Birth:** September 12, 1990  
**Address:** 1537 Riverside Avenue Unit 12, Seattle, WA 98101  
**Phone:** (206) 555-0683  
**Email:** amandagj1990@techmail.com  
**Driver's License:** WA-DL-J648572139  
**Credit Card:** 4111 1111 1111 1111 (Exp: 10/26, CVV: 789)  
**Bank Account:** US Bank - 7890123456  
**Occupation:** Graphic Designer  
**Annual Income:** $58,900  
"""

VALIDATION_PROMPT = """NEED TO WRITE IT"""


#TODO 1:
# Create AzureChatOpenAI client, model to use `gpt-4.1-nano-2025-04-14` (or any other mini or nano models)

def validate(user_input: str):
    #TODO 2:
    # Make validation of user input on possible manipulations, jailbreaks, prompt injections, etc.
    # I would recommend to use Langchain for that: PydanticOutputParser + ChatPromptTemplate (prompt | client | parser -> invoke)
    # I would recommend this video to watch to understand how to do that https://www.youtube.com/watch?v=R0RwdOc338w
    # ---
    # Hint 1: You need to write properly VALIDATION_PROMPT
    # Hint 2: Create pydentic model for validation
    raise NotImplementedError

def main():
    #TODO 1:
    # 1. Create messages array with system prompt as 1st message and user message with PROFILE info (we emulate the
    #    flow when we retrieved PII from some DB and put it as user message).
    # 2. Create console chat with LLM, preserve history there. In chat there are should be preserved such flow:
    #    -> user input -> validation of user input -> valid -> generation -> response to user
    #                                              -> invalid -> reject with reason
    raise NotImplementedError


main()

#TODO:
# ---------
# Create guardrail that will prevent prompt injections with user query (input guardrail).
# Flow:
#    -> user query
#    -> injections validation by LLM:
#       Not found: call LLM with message history, add response to history and print to console
#       Found: block such request and inform user.
# Such guardrail is quite efficient for simple strategies of prompt injections, but it won't always work for some
# complicated, multi-step strategies.
# ---------
# 1. Complete all to do from above
# 2. Run application and try to get Amanda's PII (use approaches from previous task)
#    Injections to try ðŸ‘‰ tasks.PROMPT_INJECTIONS_TO_TEST.md
